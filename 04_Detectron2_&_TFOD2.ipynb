{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Detectron2 and how does it differ from previous object detection\n",
        "frameworks?\n",
        "   - Detectron2 is a modern, open source object detection and computer vision framework developed by Facebook AI Research that makes it easier and faster to build high performance models for tasks like object detection, instance segmentation, keypoint detection, and panoptic segmentation. What sets Detectron2 apart from earlier object detection frameworks is its clean, modular design built on PyTorch, which allows researchers and developers to customize models, experiment with new ideas, and debug more easily. Unlike older frameworks that were often rigid, harder to extend, or based on static computation graphs, Detectron2 supports dynamic graphs, provides state of the art pre trained models, offers better training speed and scalability, and integrates smoothly with modern deep learning workflows, making it more flexible, user friendly, and suitable for both research and production use\n",
        "\n",
        "2. Explain the process and importance of data annotation when working with\n",
        "Detectron2.\n",
        "    - Data annotation is the process of labeling raw images with meaningful information such as bounding boxes, class labels, segmentation masks, or keypoints so that Detectron2 models can learn to recognize and localize objects correctly. When working with Detectron2, annotated data is usually prepared in standard formats like COCO, where each object in an image is precisely marked and described, allowing the framework to understand what to learn and how to evaluate predictions. This process is extremely important because the quality of annotations directly affects model performance accurate, consistent labels help the model learn correct patterns, while poor or inconsistent annotations can lead to incorrect detections and weak generalization. In short, good data annotation acts as the foundation for training reliable and high performing Detectron2 models, making it just as critical as choosing the right algorithm or architecture.\n",
        "\n",
        "3. Describe the steps involved in training a custom object detection model\n",
        "using Detectron2.\n",
        "   - Training a custom object detection model using Detectron2 involves a clear sequence of steps that turn raw images into a working, accurate model. First, the dataset is collected and carefully annotated with labels such as bounding boxes or masks, usually in COCO format, so Detectron2 can read it correctly. Next, the dataset is registered within Detectron2, where class names and paths to images and annotations are defined. After that, a suitable pre trained model and configuration file are selected to benefit from transfer learning, which helps the model learn faster and perform better with limited data. The configuration is then customized by setting parameters like the number of classes, learning rate, batch size, and training iterations. Once everything is set, the model is trained using Detectron2’s training engine, during which it learns object features from the annotated data. Finally, the trained model is evaluated on validation data and fine tuned if needed, ensuring it performs well before being used for real world object detection tasks.\n",
        "\n",
        "4. What are evaluation curves in Detectron2, and how are metrics like mAP\n",
        "and IoU interpreted?\n",
        "   - Evaluation curves in Detectron2 are visual and numerical tools used to measure how well an object detection model is performing during and after training. These curves and metrics help compare predicted results with ground truth annotations to understand the model’s accuracy and reliability. One of the most important metrics is Intersection over Union , which measures how much the predicted bounding box overlaps with the actual object box, with higher IoU values indicating better localization accuracy. Mean Average Precision builds on IoU by combining precision and recall across different confidence thresholds and object classes, giving a single score that summarizes overall detection performance. In simple terms, higher mAP values mean the model is detecting objects more accurately and consistently, while evaluation curves allow developers to track improvements, spot overfitting, and make informed decisions about model tuning.\n",
        "\n",
        "5. Compare Detectron2 and TFOD2 in terms of features, performance, and\n",
        "ease of use.\n",
        "   - Detectron2 and TensorFlow Object Detection API (TFOD2) are both powerful frameworks for object detection, but they differ in design philosophy, performance, and ease of use. Detectron2, built on PyTorch, is highly modular and flexible, making it ideal for research and rapid experimentation with advanced models like Faster RCNN, Mask RCNN, and panoptic segmentation, and it generally offers strong performance with clean, readable code and easier debugging due to PyTorch’s dynamic computation graph. TFOD2, built on TensorFlow, provides a wide range of pre trained models and strong deployment support through TensorFlow Serving, TensorFlow Lite, and TensorFlow.js, which makes it more suitable for production and mobile or edge applications. In terms of ease of use, Detectron2 is often considered simpler and more intuitive for developers familiar with PyTorch, while TFOD2 can feel more complex because of its configuration heavy setup but offers better integration with TensorFlow’s deployment ecosystem, so the choice depends on whether the focus is research flexibility or large scale production deployment."
      ],
      "metadata": {
        "id": "WZu-7g6k6yBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch torchvision torchaudio\n",
        "!pip install -U 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B21NRlxpBAGZ",
        "outputId": "fecb43f7-ab1e-403c-ef75-b5a41521ff1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Collecting torch\n",
            "  Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting cuda-bindings==12.9.4 (from torch)\n",
            "  Downloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.4.5 (from torch)\n",
            "  Downloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.6.0 (from torch)\n",
            "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch)\n",
            "  Downloading cuda_pathfinder-1.3.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m980.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cuda_pathfinder-1.3.3-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, cuda-pathfinder, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, cuda-bindings, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.29.2\n",
            "    Uninstalling nvidia-nccl-cu12-2.29.2:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.29.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cpu\n",
            "    Uninstalling torch-2.9.0+cpu:\n",
            "      Successfully uninstalled torch-2.9.0+cpu\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cpu\n",
            "    Uninstalling torchvision-0.24.0+cpu:\n",
            "      Successfully uninstalled torchvision-0.24.0+cpu\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cpu\n",
            "    Uninstalling torchaudio-2.9.0+cpu:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cpu\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cuda-bindings-12.9.4 cuda-pathfinder-1.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 torch-2.10.0 torchaudio-2.10.0 torchvision-0.25.0 triton-3.6.0\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-solcta23\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-solcta23\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit fd27788985af0f4ca800bca563acdb700bb890e2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.11)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.3.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-26.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.9/88.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.1)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=1.0.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-1.0.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.5.1)\n",
            "Collecting pytokens>=0.3.0 (from black->detectron2==0.6)\n",
            "  Downloading pytokens-0.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.5)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-26.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-1.0.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytokens-0.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.9/268.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp312-cp312-linux_x86_64.whl size=6734852 sha256=16e7746e8cf4c0bdb410a3feac61d82bf147dd70e71fde8e078d7d2a4f8473ab\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-16uh330w/wheels/d3/6e/bd/1969578f1456a6be2d6f083da65c669f450b23b8f3d1ac14c1\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=d3df8d27d9cb25ce47b759301207a819769107fe0ec72155ab247a1b2562591a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: yacs, pytokens, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed black-26.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-1.0.3 portalocker-3.2.0 pytokens-0.4.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAZnAv3m6w-I",
        "outputId": "2539306e-6b08-44d1-d6f2-b66f8f1c0920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detectron2 version: 0.6\n",
            "Detectron2 installed successfully!\n"
          ]
        }
      ],
      "source": [
        "#  6 Write Python code to install Detectron2 and verify the installation.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "# !pip install detectron2\n",
        "\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "\n",
        "setup_logger()\n",
        "print(\"Detectron2 version:\", detectron2.__version__)\n",
        "print(\"Detectron2 installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Annotate a dataset using any tool of your choice and convert the\n",
        "# annotations to COCO format for Detectron2.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "image_dir = \"sample_data\"\n",
        "annotation_dir = \"sample_data\"\n",
        "\n",
        "coco = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": [{\"id\": 1, \"name\": \"person\"}]\n",
        "}\n",
        "\n",
        "annotation_id = 1\n",
        "image_id = 1\n",
        "\n",
        "for file in os.listdir(annotation_dir):\n",
        "    if not file.endswith(\".xml\"):\n",
        "        continue\n",
        "\n",
        "    tree = ET.parse(os.path.join(annotation_dir, file))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    filename = root.find(\"filename\").text\n",
        "    width = int(root.find(\"size/width\").text)\n",
        "    height = int(root.find(\"size/height\").text)\n",
        "\n",
        "    coco[\"images\"].append({\n",
        "        \"id\": image_id,\n",
        "        \"file_name\": filename,\n",
        "        \"width\": width,\n",
        "        \"height\": height\n",
        "    })\n",
        "\n",
        "    for obj in root.findall(\"object\"):\n",
        "        bbox = obj.find(\"bndbox\")\n",
        "        xmin = int(bbox.find(\"xmin\").text)\n",
        "        ymin = int(bbox.find(\"ymin\").text)\n",
        "        xmax = int(bbox.find(\"xmax\").text)\n",
        "        ymax = int(bbox.find(\"ymax\").text)\n",
        "\n",
        "        coco[\"annotations\"].append({\n",
        "            \"id\": annotation_id,\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": 1,\n",
        "            \"bbox\": [xmin, ymin, xmax - xmin, ymax - ymin],\n",
        "            \"area\": (xmax - xmin) * (ymax - ymin),\n",
        "            \"iscrowd\": 0\n",
        "        })\n",
        "\n",
        "        annotation_id += 1\n",
        "\n",
        "    image_id += 1\n",
        "\n",
        "with open(\"/content/annotations_coco.json\", \"w\") as f:\n",
        "    json.dump(coco, f, indent=4)\n",
        "\n",
        "print(\"COCO annotation file created successfully\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ah9LmUToA5_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbd458e-6833-49d2-d857-131390067142"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COCO annotation file created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 Write a script to download pretrained weights and configure paths for\n",
        "# training in Detectron2.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\n",
        "    model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        ")\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"custom_train\",)\n",
        "cfg.DATASETS.TEST = (\"custom_val\",)\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        ")\n",
        "\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "\n",
        "cfg.OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Pretrained weights path:\")\n",
        "print(cfg.MODEL.WEIGHTS)\n",
        "print(\"Output directory configured at:\", cfg.OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7xa0MOuCgUS",
        "outputId": "86a28666-7e5a-4bb7-86ce-1d6535d3fdae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights path:\n",
            "https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\n",
            "Output directory configured at: ./output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Show the steps and code to run inference using a trained Detectron2\n",
        "# model on a new image.\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "import cv2\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\n",
        "    model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        ")\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        ")\n",
        "cfg.MODEL.SCORE_THRESH_TEST = 0.5\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "outputs = predictor(image)\n",
        "\n",
        "print(\"Detected instances:\", len(outputs[\"instances\"]))\n",
        "print(\"Predicted boxes:\\n\", outputs[\"instances\"].pred_boxes)\n"
      ],
      "metadata": {
        "id": "wFwACy54CnXy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "233ac849-caac-4f05-c842-3af480c77017"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-32bad019-167a-489f-9603-a74b7e9333d8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-32bad019-167a-489f-9603-a74b7e9333d8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving d.jpg to d (1).jpg\n",
            "[01/23 13:57:12 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_280758.pkl: 167MB [00:01, 111MB/s]                           \n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4381.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "W0123 13:57:25.939000 670 torch/fx/_symbolic_trace.py:53] is_fx_tracing will return true for both fx.symbolic_trace and torch.export. Please use is_fx_tracing_symbolic_tracing() for specifically fx.symbolic_trace or torch.compiler.is_compiling() for specifically torch.export/compile.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected instances: 100\n",
            "Predicted boxes:\n",
            " Boxes(tensor([[1.8256e+02, 3.8094e+01, 1.9801e+02, 8.7197e+01],\n",
            "        [1.4638e+02, 7.5613e+01, 1.6041e+02, 1.1970e+02],\n",
            "        [0.0000e+00, 1.0489e+02, 8.7567e+01, 1.7500e+02],\n",
            "        [1.6307e+02, 3.7212e+01, 1.7951e+02, 8.4322e+01],\n",
            "        [3.0131e+01, 1.0021e+02, 7.3456e+01, 1.7493e+02],\n",
            "        [0.0000e+00, 4.7488e-02, 2.2956e+01, 5.9849e+01],\n",
            "        [5.2355e+01, 4.0150e+01, 7.1891e+01, 9.3929e+01],\n",
            "        [6.0921e+01, 3.9693e+01, 7.9495e+01, 9.5589e+01],\n",
            "        [1.6729e+02, 3.3939e+01, 1.9000e+02, 7.8109e+01],\n",
            "        [3.2971e+01, 1.1344e+02, 5.7306e+01, 1.6758e+02],\n",
            "        [0.0000e+00, 7.8529e+01, 2.2337e+01, 1.7500e+02],\n",
            "        [1.5546e+02, 6.8899e+01, 1.7329e+02, 1.0915e+02],\n",
            "        [4.6342e+01, 7.1582e+01, 1.1626e+02, 8.9090e+01],\n",
            "        [1.4843e+02, 4.6800e+01, 1.6796e+02, 9.7646e+01],\n",
            "        [1.7631e+02, 3.8655e+01, 1.9321e+02, 8.0230e+01],\n",
            "        [4.0936e+01, 6.5768e+01, 1.3047e+02, 7.8361e+01],\n",
            "        [1.3427e+02, 7.8935e+01, 1.7833e+02, 9.6710e+01],\n",
            "        [1.5233e+02, 8.8267e+01, 1.6074e+02, 1.2008e+02],\n",
            "        [1.4116e+02, 7.2828e+01, 1.5833e+02, 1.0587e+02],\n",
            "        [1.4778e+02, 3.0899e+01, 2.1161e+02, 1.4195e+02],\n",
            "        [4.8554e+01, 6.5111e+01, 6.2769e+01, 8.3573e+01],\n",
            "        [6.6794e+01, 4.4597e+01, 8.6523e+01, 9.8784e+01],\n",
            "        [1.5158e+02, 8.0073e+01, 1.7111e+02, 1.1533e+02],\n",
            "        [2.6585e+02, 4.2578e-01, 2.8892e+02, 1.7500e+02],\n",
            "        [5.2329e+01, 4.7693e+01, 9.3624e+01, 9.0031e+01],\n",
            "        [2.4971e+01, 1.0737e+02, 1.2639e+02, 1.6636e+02],\n",
            "        [3.9663e+01, 1.1092e+02, 1.1053e+02, 1.2568e+02],\n",
            "        [7.5934e+01, 6.1946e+01, 9.4059e+01, 1.1238e+02],\n",
            "        [7.5919e+01, 1.0187e+02, 9.9197e+01, 1.4419e+02],\n",
            "        [7.1005e+01, 1.0275e+02, 9.1630e+01, 1.5414e+02],\n",
            "        [3.9858e+01, 5.6997e+01, 1.4282e+02, 6.9683e+01],\n",
            "        [4.7663e+01, 5.7943e+01, 8.1894e+01, 9.3204e+01],\n",
            "        [3.0400e-01, 1.4725e+02, 3.1495e+01, 1.7369e+02],\n",
            "        [1.5777e+02, 5.7318e+01, 2.4029e+02, 1.6097e+02],\n",
            "        [0.0000e+00, 5.0993e+01, 7.6759e+01, 1.6521e+02],\n",
            "        [1.2997e+02, 1.1112e+02, 1.4295e+02, 1.2263e+02],\n",
            "        [4.7131e+01, 9.8021e+01, 7.0640e+01, 1.5383e+02],\n",
            "        [1.4841e+02, 9.2602e+01, 1.5879e+02, 1.1940e+02],\n",
            "        [1.1240e+02, 9.1543e+01, 1.4658e+02, 1.2328e+02],\n",
            "        [1.4844e+02, 8.3564e+01, 1.5879e+02, 1.0214e+02],\n",
            "        [4.8801e+01, 6.9090e+01, 1.3861e+02, 8.2165e+01],\n",
            "        [6.0112e+01, 6.0766e+01, 6.9162e+01, 8.6091e+01],\n",
            "        [4.7674e+01, 4.3656e+01, 6.5152e+01, 9.1002e+01],\n",
            "        [4.9068e+01, 0.0000e+00, 2.4518e+02, 5.0522e+01],\n",
            "        [1.2628e+02, 2.3531e+01, 1.8226e+02, 1.2268e+02],\n",
            "        [1.2477e+02, 1.0853e+02, 1.3807e+02, 1.2259e+02],\n",
            "        [1.6307e+02, 1.0406e+02, 2.6361e+02, 1.7500e+02],\n",
            "        [9.5652e+01, 1.5861e+02, 1.7128e+02, 1.7273e+02],\n",
            "        [4.0844e+01, 1.0941e+02, 6.3392e+01, 1.6464e+02],\n",
            "        [2.9604e+01, 1.4671e+02, 6.1715e+01, 1.7436e+02],\n",
            "        [5.7826e+01, 2.3230e+01, 7.3400e+01, 3.9567e+01],\n",
            "        [0.0000e+00, 1.6480e-01, 1.7263e+01, 1.7500e+02],\n",
            "        [4.8012e+01, 1.1020e+02, 6.4787e+01, 1.2158e+02],\n",
            "        [1.3834e+01, 1.3356e+02, 1.4126e+02, 1.7127e+02],\n",
            "        [1.8412e+02, 4.2663e+01, 1.9298e+02, 6.8621e+01],\n",
            "        [4.6772e+01, 0.0000e+00, 9.9605e+01, 9.6287e+01],\n",
            "        [5.9437e+01, 1.0466e+02, 9.6323e+01, 1.4096e+02],\n",
            "        [5.4257e+01, 6.0191e+01, 6.4198e+01, 8.5867e+01],\n",
            "        [2.0130e+00, 1.3167e+02, 4.3147e+01, 1.7125e+02],\n",
            "        [1.7468e+02, 1.0130e+02, 1.9364e+02, 1.1924e+02],\n",
            "        [5.5792e+01, 1.0766e+02, 7.2341e+01, 1.6712e+02],\n",
            "        [8.0024e+01, 1.1584e+02, 1.1384e+02, 1.5176e+02],\n",
            "        [5.4884e+01, 2.3336e+01, 6.6478e+01, 4.6293e+01],\n",
            "        [9.3672e+01, 7.9632e+01, 1.0559e+02, 1.0018e+02],\n",
            "        [1.4486e+02, 2.8594e+01, 2.5603e+02, 1.0070e+02],\n",
            "        [5.0478e+01, 4.0057e+01, 1.2037e+02, 1.0797e+02],\n",
            "        [1.0148e+02, 7.8489e+01, 1.2865e+02, 1.0672e+02],\n",
            "        [1.6629e+02, 2.3738e-01, 2.8900e+02, 1.7500e+02],\n",
            "        [1.5225e+02, 8.2040e+01, 1.6422e+02, 9.6974e+01],\n",
            "        [3.7877e+01, 5.3532e+01, 5.3082e+01, 6.7270e+01],\n",
            "        [4.4771e+01, 1.0611e+02, 2.7315e+02, 1.7289e+02],\n",
            "        [4.7406e+01, 1.0905e+02, 1.4934e+02, 1.2302e+02],\n",
            "        [1.5441e+02, 7.7552e+01, 2.7084e+02, 1.4161e+02],\n",
            "        [0.0000e+00, 3.1328e-01, 3.9258e+01, 1.7500e+02],\n",
            "        [2.8220e+01, 1.2803e+02, 4.9803e+01, 1.7240e+02],\n",
            "        [6.1020e+01, 1.1029e+02, 7.8131e+01, 1.6491e+02],\n",
            "        [1.3949e+02, 9.4323e+01, 1.5200e+02, 1.2067e+02],\n",
            "        [1.5093e+02, 5.6790e+01, 1.9550e+02, 1.7004e+02],\n",
            "        [1.4746e+02, 7.7766e+01, 1.5706e+02, 9.4940e+01],\n",
            "        [1.0773e+02, 8.4155e+01, 1.3722e+02, 1.1587e+02],\n",
            "        [6.8889e+01, 9.5592e+01, 1.2690e+02, 1.1162e+02],\n",
            "        [1.5109e+02, 8.3277e+01, 1.5700e+02, 9.4310e+01],\n",
            "        [1.0815e+02, 8.4127e+01, 1.1371e+02, 9.6365e+01],\n",
            "        [6.9442e+01, 0.0000e+00, 2.8900e+02, 1.5087e+01],\n",
            "        [1.3185e+02, 1.1149e+02, 1.5203e+02, 1.2267e+02],\n",
            "        [1.2466e+02, 7.7211e+01, 1.6062e+02, 1.1585e+02],\n",
            "        [5.3069e+01, 6.1183e+01, 7.0174e+01, 9.8112e+01],\n",
            "        [1.2476e+02, 2.7592e+00, 1.9613e+02, 8.0623e+01],\n",
            "        [1.5298e+02, 8.4315e+01, 1.5874e+02, 9.4759e+01],\n",
            "        [2.0983e-01, 0.0000e+00, 1.0403e+02, 6.5268e+01],\n",
            "        [4.7720e+01, 7.4779e+01, 6.9023e+01, 1.0779e+02],\n",
            "        [1.3969e+01, 5.4928e+01, 1.2445e+02, 9.9669e+01],\n",
            "        [4.3703e+01, 4.4468e+01, 5.5174e+01, 6.5568e+01],\n",
            "        [1.3204e+02, 1.1544e+02, 1.3909e+02, 1.2203e+02],\n",
            "        [5.8468e+01, 1.3079e+01, 2.6134e+02, 1.7487e+02],\n",
            "        [7.1470e+01, 1.0806e+00, 1.1416e+02, 2.0211e+01],\n",
            "        [4.3464e+01, 4.5301e+01, 5.8519e+01, 9.0596e+01],\n",
            "        [4.6206e+01, 4.3907e+01, 6.4007e+01, 6.2717e+01],\n",
            "        [8.9293e+01, 8.8874e+01, 9.8947e+01, 1.1241e+02],\n",
            "        [1.7084e+02, 1.2929e+02, 2.8156e+02, 1.6779e+02]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are assigned to build a wildlife monitoring system to detect and track\n",
        "different animal species in a forest using Detectron2. Describe the end-to-end pipeline\n",
        "from data collection to deploying the model, and how you would handle challenges like\n",
        "occlusion or nighttime detection.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "    - Building a wildlife monitoring system with Detectron2 involves an end-to-end pipeline that connects real world forest data to a deployed, intelligent detection system. The process starts with data collection using camera traps, drones, or fixed surveillance cameras placed across different forest locations to capture animals under varied conditions such as daylight, nighttime, rain, and dense vegetation. These images and videos are then carefully annotated using tools like CVAT or LabelImg, labeling animal species with bounding boxes or segmentation masks and converting them into COCO format for Detectron2. Next, the dataset is registered and a suitable pre-trained model is fine tuned using Detectron2, allowing the model to learn species specific features while benefiting from transfer learning. During training, challenges like occlusion are handled by using diverse data with partial visibility, data augmentation, and instance segmentation models that better separate overlapping animals, while nighttime detection is improved by including infrared images, low light samples, and brightness/contrast augmentation. After training and evaluation using metrics like MAP and IOU, the optimized model is deployed on edge devices or servers, where it performs real time inference and tracking, logs detections, and supports conservation decisions through dashboards or alerts."
      ],
      "metadata": {
        "id": "SVrqgQW0DK4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "import cv2\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\n",
        "    model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        ")\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "cfg.MODEL.WEIGHTS = \"./model_final.pth\"\n",
        "cfg.MODEL.SCORE_THRESH_TEST = 0.6\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "image = cv2.imread(\"forest_night.jpg\")\n",
        "outputs = predictor(image)\n",
        "\n",
        "print(\"Detected animals:\", len(outputs[\"instances\"]))\n",
        "print(\"Predicted classes:\", outputs[\"instances\"].pred_classes.tolist())\n"
      ],
      "metadata": {
        "id": "g38m7APvC76g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BR92EJ_rFeGR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}